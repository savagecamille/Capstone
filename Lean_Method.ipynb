{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde08b57",
   "metadata": {},
   "source": [
    "## Camille Hansen \n",
    "# A NETWORK MODELLING BASED SOCIAL BOT DETECTION   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704be961",
   "metadata": {},
   "source": [
    "# Set Up \n",
    "\n",
    "In juypter Notebook bash commands can be run directly in the notebook by using '!'. \n",
    "I used this to install Tweepy and Pandas and python-igraph  using \"!pip install\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407a829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import Packages\n",
    "import tweepy \n",
    "import pandas as pd\n",
    "import pickle \n",
    "#import twitter  #used for trouble shooting - ultimately tweepy was prefered for the twitter API \n",
    "import numpy as np\n",
    "import nextworkx\n",
    "import igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81653297",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Twitter Developer Credentials \n",
    "# actual keys hidden for privacy and accountability reasons\n",
    "consumer_key = \"consumer_key\"\n",
    "consumer_secret = \"consumer_secret_key\"\n",
    "access_token = \"access_token\"\n",
    "access_token_secret = \"access_token_secret\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e69a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tweepy Authorisation \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api_tweepy = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92c291",
   "metadata": {},
   "source": [
    "# Data Mining \n",
    "\n",
    "## Mining Root User\n",
    "Get Twitter Data about the choosen root user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcee525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from the fakedataset check the user name for the user is current using the users id (a unique value twitter creates that won't change over time or be reused by new accounts)\n",
    "u = api_tweepy.get_user(id=123456) \n",
    "print(u) #read the screen_name from this infomation (it will give the current screen name of the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf918223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the root user by entering their screen_name as a variable\n",
    "screen_name = \"screen_name\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d423b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Only need to Run Once - NOT RUN WHEN STARTING A NEW SESSION \n",
    "\n",
    "# Using tweepy's twitter api to mine the users twitter data as an object class of the User \n",
    "user = api_tweepy.get_user(screen_name)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899aa453",
   "metadata": {},
   "source": [
    "## Mining Friends of Root User \n",
    "Forming a list of the root users friends and storing it as a pickled file.\n",
    "I need to store the data as the CPU and time heavy data mining process is not something I want to repeat. \n",
    "I choose pickle over CSV because pickle is a serialized way of storing a Pandas dataframe meaning I am writing down the exact representation of the dataframe to disk. CSV stores a comma separated list meaning some information may be lost when you loading it in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Only need to Run Once - NOT RUN WHEN STARTING A NEW SESSION \n",
    "\n",
    "#Creating an empty list that will be filled with the root users friends \n",
    "rootUser = []\n",
    "\n",
    "#Mining the root users friends \n",
    "for friend in tweepy.Cursor(api_tweepy.friends_ids, screen_name).items():\n",
    "    rootUser.append(friend)\n",
    "\n",
    "#The Friend List is structured in a pandas dataframe and stored externally in a pickled file\n",
    "root_user_df = pd.DataFrame({'screen_name':rootUser})\n",
    "root_user_df.to_pickle('screen_name_rootUser_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b589f9",
   "metadata": {},
   "source": [
    "### Output \n",
    "A list of the root user's friends in a data frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47252409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Starting a New Session - RUN IN CONSECUTIVE SESSIONS - read pre-generated pkl files \n",
    "\n",
    "#Read The Pickled File \n",
    "root_user_df = pd.read_pickle('screen_name_rootUser_df.pkl')\n",
    "root_user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661013",
   "metadata": {},
   "source": [
    "## Mining Friends of Friends\n",
    "Mining the data needed for a **1-step Neighbourhood** by extending the friends dataframe to include the friends of freinds that appear in the root users friend list. \n",
    "\n",
    "This data frame is shorter than the above one as some users were ristricted and had to be removed as their data could not be minned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Only need to Run till completion of fof data mining - NOT RUN WHEN STARTING A NEW SESSION \n",
    "\n",
    "#Make the friend list dataframe a list \n",
    "user_List = root_user_df['screen_name'].values.tolist()\n",
    "#Copy this list to iterate through and edit for mining \n",
    "user_Active = user_List\n",
    "\n",
    "#empty freinds of friends dataframe \n",
    "fof_df = pd.DataFrame()\n",
    "    \n",
    "while len(user_Active)>0: \n",
    "    for friend_id in user_Active:\n",
    "        \n",
    "        try: \n",
    "            friends_list = []  #empty list to add to \n",
    "            col_name = str(friend_id)    #naming the saved files to disk with variable names \n",
    "            \n",
    "            for fof in tweepy.Cursor(api_tweepy.friends_ids, id = friend_id).items(): #get friend of friend IDs using tweepy\n",
    "                for i in user_List: #only including friend_IDs that are from the neighbourhood\n",
    "                    if fof == i:\n",
    "                        friends_list.append(fof)\n",
    "\n",
    "            while len(friends_list) < len(user_List): #padding shorter lists to prevent value error with the number of rows from root_user_df\n",
    "                friends_list.extend([''])\n",
    "                \n",
    "            fof_df[friend_id] = friends_list #add the list to the data frame with the column heading friend_id\n",
    "            \n",
    "            fof_df.to_pickle(col_name + 'screen_name_fof_df.pkl') #all pickled files before the most recent become redundant (and are deleted manually) but performing this for each column protects against unexpected system crashes\n",
    "           # fof_df.to_csv(col_name + 'fof_df.csv')    My way of checking it was working like expected without interrupting the program\n",
    "\n",
    "            friends_list.clear() \n",
    "            user_Active.remove(friend_id)\n",
    "            \n",
    "        except tweepy.TweepError as e:  #skipping over accounts that are restricted and documenting when this error occurs \n",
    "            print(e)\n",
    "            print('problem at %s'%len(user_Active))\n",
    "            print('Account %s'%friend_id)\n",
    "            user_Active.pop(0)\n",
    "        \n",
    "    print(len(user_Active))   #counting down accounts left - this is imporant as this process took over 6 days to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e75750",
   "metadata": {},
   "outputs": [],
   "source": [
    "fof_df_final = pd.read_pickle('final_fof_df.pkl') #the final fof dataframe \n",
    "fof_df_final.to_pickle('screen_name_fof_df_final.pkl')  #renameing it for ease of documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e6f63",
   "metadata": {},
   "source": [
    "### Output \n",
    "A dataframe of the friends of friends  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Starting a New Session - RUN IN CONSECUTIVE SESSIONS - read pre-generated pkl files \n",
    "\n",
    "fof_df_final = pd.read_pickle('screen_name_fof_df_final.pkl') \n",
    "fof_df_final #showing the pandas dataframe of screenuser friends of friends "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff0c76",
   "metadata": {},
   "source": [
    "# Creating 1-Step-Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8636241",
   "metadata": {},
   "source": [
    "### Removing Restricted Accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44404b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing restricted accounts from the friends list and making the root users friend list only include the accounts that could have their friends of friends retrieved\n",
    "user_List = root_user_df['screen_name'].values.tolist()\n",
    "allowed_friends = list(fof_df_final.columns.values) #making column headings a list (these are the accounts that were not restricted)\n",
    "updated_user = list(set(user_List).intersection(set(allowed_friends))) #updating user_List with the accounts that appear in both lists\n",
    "updated_user_df = pd.DataFrame({'screen_name':updated_user}) #updating root user without restricted accounts \n",
    "updated_user_df #show to verify "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558aaef",
   "metadata": {},
   "source": [
    "### Forming the Complete DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data frames \n",
    "df_complete = updated_user_df.join(fof_df_final) #combine root user and fof dataframes \n",
    "df_complete = df_complete.fillna(value=0) #remove any NaN values with 0 - to support list iteration later on \n",
    "df_complete.to_pickle('screen_name_df_complete.pkl') #store to call upon later \n",
    "df_complete.to_csv('screen_name_df_complete.csv') # store to manually observe on personal device \n",
    "df_complete #show to verify "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1136f5",
   "metadata": {},
   "source": [
    "### Output \n",
    "A dataframe of the entire 1-step Twitter Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Starting a New Session - RUN IN CONSECUTIVE SESSIONS - read pre-generated pkl files \n",
    "\n",
    "df_complete  = pd.read_pickle('screen_name_df_complete.pkl') \n",
    "df_complete #showing the pandas dataframe of screenuser friends of friends and screenuser friends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885d992",
   "metadata": {},
   "source": [
    "# Transforming Dataframe to an Edgelist \n",
    "Graphing was completed in Gephi. \n",
    "Gephi is a network graphing software that creates visualisations based on inputs such as an edgelist with the headings 'source', 'target' and 'weight'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd022a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to great edgeweights based on the number of common connections and if connections are reciprocal\n",
    "#this defines a common friend as 1 increase to the weight value but if their relationship is reciprocated each friend becomes exponentially more important\n",
    "#these weight were not implimented in Gephi graphs as degree (number of connections) created a similar statistical definition and extreme  variations in weight created less succinct community modelling \n",
    "#improving the weight attribute will be the next step in improving this method of bot detection and is discussed in detail in the associated report \n",
    "def get_weight(col, value): \n",
    "    b = [] #empty list that will be used to append friends of user b \n",
    "    a = [] #empty list that will be used to append friends of user a\n",
    "    w = 1 #set default weight to 1\n",
    "    if col != 'screen_name': \n",
    "         col = int(col) #format string number value as int number value (consistent with the format of the df_complete dataframe)\n",
    "    if value != 'screen_name' and value != '':\n",
    "          value = int(value) #format string number value as int number value (consistent format of the df_complete dataframe)\n",
    "    a = df_complete[col].values.tolist() #let a be the friends of user a\n",
    "    a_list = [ele for ele in a if ele !='' ] #let a_list be the friends of user a if they are not '' (a blank padding value)\n",
    "    b_list = []\n",
    "    if value != '':\n",
    "        try: \n",
    "            b = df_complete[value].values.tolist()  #let a be the friends of user b\n",
    "            b_list = [ele for ele in b if ele !='' ] #let b_list be the friends of user b if they are not '' (a blank padding value)\n",
    "        except KeyError as e:  \n",
    "            print(e)\n",
    "    if col in b_list: #if relationship is reciprocated \n",
    "        w = 3*(len(np.intersect1d(a,b))) #weight = exponential of number of common freinds \n",
    "    else:  #if not reciprocated \n",
    "        w = 2*len(np.intersect1d(a,b)) #weight = number of common freinds\n",
    "        if w == 0: #remove 0 weights (upsets graphing)\n",
    "            w = 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19192e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edgelist = pd.DataFrame(columns = ['source', 'target', 'weight']) #empty dataframe with headings as defined\n",
    "w = 1 \n",
    "for col in df_complete:\n",
    "    for value in df_complete[col].tolist(): \n",
    "            if value != \"\": #removing padding \n",
    "                if value != 0: #removing NaN values\n",
    "                    col = str(col) #consistent with format of the edgelist desired by Gephi \n",
    "                    value = str(value)#consistent with format of the edgelist desired by Gephi \n",
    "                    new_row = {'source': col, 'target': value, 'weight': get_weight(col,value)} #append row calling the get_weight function to determine the weight\n",
    "                    df_edgelist = df_edgelist.append(new_row, ignore_index = True) #cycling through to add each row\n",
    "            \n",
    "df_edgelist #visually verify \n",
    "df_edgelist.to_csv('screen_name_EdgeList.csv') #export to csv for Gephi \n",
    "df_edgelist.to_pickle('screen_name_Edgelist.pkl') #store as pickle to reread in python for networkx and igraph modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d894c",
   "metadata": {},
   "source": [
    "### Output \n",
    "A Edgelist data frame of the source and target node with a generated weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Starting a New Session - RUN IN CONSECUTIVE SESSIONS - read pre-generated pkl files \n",
    "\n",
    "df_complete  = pd.read_pickle('screen_name_Edgelist.pkl') \n",
    "df_edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5e3bc",
   "metadata": {},
   "source": [
    "This edgelist was then transfered to Gephi for network modelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e76d0d",
   "metadata": {},
   "source": [
    "# Networkx and IGraph verification of statitical findings sourced from Gephi\n",
    "### Statistical Graph Values exploited for Bot Detection \n",
    "Through studying the graph in gephi I identifid a trend between spambots, socialbots and genuine users.\n",
    "The trend appeared in the Average Degree attribute and Clustering Triangles attribute. \n",
    "Here I verify those numbers in IGraph and Networkx to validate the findings. \n",
    "#### These values where identical to those in Gephi verifying the findings across modelling softwares. This is unsuprising given average degree and number of triangles are not based on the modelling position of the graph, but rather the result of a statistical calculation, but still worth testing across modelling softwares to further validate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9729d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('screen_name_Edgelist2.txt', 'rb') as file:\n",
    "    gx_dir = nx.read_edgelist(file, create_using=nx.DiGraph(), data=(('weight',int),), delimiter=' ')\n",
    "with open('screen_name_Edgelist2.txt', 'rb') as file: \n",
    "    gx_und = nx.read_edgelist(file, create_using=nx.Graph(), data=(('weight',int),), delimiter=' ')\n",
    "with open('screen_name_Edgelist2.txt', 'r', encoding = 'utf-8') as input_file:\n",
    "    ig_dir = igraph.Graph.Read_Ncol(input_file, names=True, directed = True, weights = True)\n",
    "with open('screen_name_Edgelist2.txt', 'r', encoding = 'utf-8') as input_file:\n",
    "    ig_und = igraph.Graph.Read_Ncol(input_file, names=True, directed = False, weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangles(g):\n",
    "    cliques = g.cliques(min=3, max=3)\n",
    "    result = [0] * g.vcount()\n",
    "    for i, j, k in cliques:\n",
    "        result[i] += 1\n",
    "        result[j] += 1\n",
    "        result[k] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415ca5c",
   "metadata": {},
   "source": [
    "### Output \n",
    "Networkx driven caluclations of number of nodes, number of edges, number of triangles and average in-degree and out-degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24817761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Networkx Attributes \n",
    "gx_dir.number_of_nodes()\n",
    "gx_dir.number_of_edges()\n",
    "gx_dir.degree()\n",
    "number_of_triangles = sum(nx.triangles(gx_und).values()) / 3 #triangles can only be calculated in an undirected graph\n",
    "print('\\n screen_name:')\n",
    "print(nx.info(gx_dir)) \n",
    "print('Total Triangles:  %.2f'  %number_of_triangles )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c499d",
   "metadata": {},
   "source": [
    "### Output \n",
    "IGraph driven caluclations of number of nodes, number of edges, number of triangles and average degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGraph Attributes \n",
    "print('\\n screen_name:')\n",
    "summary(ig_dir)\n",
    "degin = ig_dir.indegree()\n",
    "avg_degin = sum(degin)/len(degin)\n",
    "degout = ig_dir.indegree()\n",
    "avg_degout = sum(degout)/len(degout)\n",
    "#ig_und.transitivity_avglocal_undirected() #average clustering coefficent undirected \n",
    "num_tri = triangles(ig_und)\n",
    "total_triangles = sum(num_tri)/3 \n",
    "print('Total Triangles:  %.2f'  %total_triangles )\n",
    "print('Average in degree::  %.2f'  %avg_degin )\n",
    "print('Average out degree::  %.2f'  %avg_degout )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
